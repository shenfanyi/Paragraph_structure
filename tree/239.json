We show that kernel-based quadrature rules for computing
integrals can be seen as a special case of random feature
expansions for positive definite kernels, for a particular
decomposition that always exists for such kernels. We provide a
theoretical analysis of the number of required samples for a
given approximation error, leading to both upper and lower
bounds that are based solely on the eigenvalues of the
associated integral operator and match up to logarithmic terms.
In particular, we show that the upper bound may be obtained from
independent and identically distributed samples from a specific
non-uniform distribution, while the lower bound if valid for any
set of points. Applying our results to kernel-based quadrature,
while our results are fairly general, we recover known upper and
lower bounds for the special cases of Sobolev spaces. Moreover,
our results extend to the more general problem of full function
approximations (beyond simply computing an integral), with
results in $L_2$- and $L_\infty$-norm that match known results
for special cases. Applying our results to random features, we
show an improvement of the number of random features needed to
preserve the generalization guarantees for learning with
Lipshitz-continuous losses.

[11, 1, ["We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels", "Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces"]]
[9, 1, []]
[6, 2, []]
[7, 2, []]
[9, 1, ["Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in $L_2$- and $L_\\infty$-norm that match known results for special cases", "Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses"]]
[8, 2, []]

Structured prediction models have been found to learn effectively from a few large examples---
sometimes even just one. Despite empirical evidence, canonical learning theory cannot guarantee 
generalization in this setting because the error bounds decrease as a function of the number of examples. 
We therefore propose new PAC-Bayesian generalization bounds for structured prediction that decrease 
as a function of both the number of examples and the size of each example. Our analysis hinges on the 
stability of joint inference and the smoothness of the data distribution. We apply our bounds to several 
common learning scenarios, including max-margin and soft-max training of Markov random fields. 
Under certain conditions, the resulting error bounds can be far more optimistic than previous results and 
can even guarantee generalization from a single large example.

[3, 2, []]
[10, 1, ["Despite empirical evidence, canonical learning theory cannot guarantee  generalization in this setting because the error bounds decrease as a function of the number of examples", "We therefore propose new PAC-Bayesian generalization bounds for structured prediction that decrease  as a function of both the number of examples and the size of each example", "We apply our bounds to several  common learning scenarios, including max-margin and soft-max training of Markov random fields", "Under certain conditions, the resulting error bounds can be far more optimistic than previous results and  can even guarantee generalization from a single large example"]]
[8, 2, []]
[7, 1, ["Our analysis hinges on the  stability of joint inference and the smoothness of the data distribution"]]
[8, 2, []]
[6, 2, []]

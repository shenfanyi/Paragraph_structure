In this paper, we introduce a novel approach, called Input
Output Kernel Regression (IOKR), for learning mappings between
structured inputs and structured outputs. The approach belongs
to the family of Output Kernel Regression methods devoted to
regression in feature space endowed with some output kernel. In
order to take into account structure in input data and benefit
from kernels in the input space as well, we use the Reproducing
Kernel Hilbert Space theory for vector-valued functions. We
first recall the ridge solution for supervised learning and then
study the regularized hinge loss-based solution used in Maximum
Margin Regression. Both models are also developed in the context
of semi-supervised setting. In addition we derive an extension
of Generalized Cross Validation for model selection in the case
of the least-square model. Finally we show the versatility of
the IOKR framework on two different problems: link prediction
seen as a structured output problem and multi-task regression
seen as a multiple and interdependent output problem.
Eventually, we present a set of detailed numerical results that
shows the relevance of the method on these two tasks.

[6, 1, []]
[7, 2, []]
[12, 1, ["The approach belongs to the family of Output Kernel Regression methods devoted to regression in feature space endowed with some output kernel", "In order to take into account structure in input data and benefit from kernels in the input space as well, we use the Reproducing Kernel Hilbert Space theory for vector-valued functions"]]
[6, 1, []]
[4, 2, []]
[6, 1, ["In addition we derive an extension of Generalized Cross Validation for model selection in the case of the least-square model"]]
[10, 1, ["Finally we show the versatility of the IOKR framework on two different problems: link prediction seen as a structured output problem and multi-task regression seen as a multiple and interdependent output problem"]]
[5, 2, []]

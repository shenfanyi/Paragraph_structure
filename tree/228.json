We study a version of the proximal gradient algorithm for which
the gradient is intractable and is approximated by Monte Carlo
methods (and in particular Markov Chain Monte Carlo). We derive
conditions on the step size and the Monte Carlo batch size under
which convergence is guaranteed: both increasing batch size and
constant batch size are considered. We also derive non-
asymptotic bounds for an averaged version. Our results cover
both the cases of biased and unbiased Monte Carlo approximation.
To support our findings, we discuss the inference of a sparse
generalized linear model with random effect and the problem of
learning the edge structure and parameters of sparse undirected
graphical models.

[5, 1, ["We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo)", "We also derive non- asymptotic bounds for an averaged version"]]
[10, 1, ["We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered"]]
[2, 2, []]
[3, 2, []]
[9, 1, ["To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models"]]

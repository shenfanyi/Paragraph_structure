A fundamental challenge in developing high-impact machine
learning technologies is balancing the need to model rich,
structured domains with the ability to scale to big data. Many
important problem areas are both richly structured and large
scale, from social and biological networks, to knowledge graphs
and the Web, to images, video, and natural language. In this
paper, we introduce two new formalisms for modeling structured
data, and show that they can both capture rich structure and
scale to big data. The first, hinge-loss Markov random fields
(HL-MRFs), is a new kind of probabilistic graphical model that
generalizes different approaches to convex inference. We unite
three approaches from the randomized algorithms, probabilistic
graphical models, and fuzzy logic communities, showing that all
three lead to the same inference objective. We then define HL-
MRFs by generalizing this unified objective. The second new
formalism, probabilistic soft logic (PSL), is a probabilistic
programming language that makes HL-MRFs easy to define using a
syntax based on first-order logic. We introduce an algorithm for
inferring most-probable variable assignments (MAP inference)
that is much more scalable than general-purpose convex
optimization methods, because it uses message passing to take
advantage of sparse dependency structures. We then show how to
learn the parameters of HL-MRFs. The learned HL-MRFs are as
accurate as analogous discrete models, but much more scalable.
Together, these algorithms enable HL-MRFs and PSL to model rich,
structured data at scales not previously possible.

[8, 1, []]
[10, 1, ["Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language", "In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data", "The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic"]]
[6, 2, []]
[9, 2, []]
[8, 1, ["We then define HL- MRFs by generalizing this unified objective", "The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable", "Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible"]]
[3, 2, []]
[9, 2, []]
[11, 1, ["The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference", "We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective", "We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures"]]
[3, 2, []]
[3, 2, []]
[5, 2, []]

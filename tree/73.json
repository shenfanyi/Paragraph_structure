We consider a learning algorithm generated by a regularization
scheme with a concave regularizer for the purpose of achieving
sparsity and good learning rates in a least squares regression
setting. The regularization is induced for linear combinations
of empirical features, constructed in the literatures of kernel
principal component analysis and kernel projection machines,
based on kernels and samples. In addition to the separability of
the involved optimization problem caused by the empirical
features, we carry out sparsity and error analysis, giving
bounds in the norm of the reproducing kernel Hilbert space,
based on a priori conditions which do not require assumptions on
sparsity in terms of any basis or system. In particular, we show
that as the concave exponent $q$ of the concave regularizer
increases to $1$, the learning ability of the algorithm
improves. Some numerical simulations for both artificial and
real MHC-peptide binding data involving the $\ell^q$ regularizer
and the SCAD penalty are presented to demonstrate the sparsity
and error analysis.

[12, 2, []]
[12, 2, []]
[18, 1, ["We consider a learning algorithm generated by a regularization scheme with a concave regularizer for the purpose of achieving sparsity and good learning rates in a least squares regression setting", "The regularization is induced for linear combinations of empirical features, constructed in the literatures of kernel principal component analysis and kernel projection machines, based on kernels and samples", "In addition to the separability of the involved optimization problem caused by the empirical features, we carry out sparsity and error analysis, giving bounds in the norm of the reproducing kernel Hilbert space, based on a priori conditions which do not require assumptions on sparsity in terms of any basis or system", "Some numerical simulations for both artificial and real MHC-peptide binding data involving the $\\ell^q$ regularizer and the SCAD penalty are presented to demonstrate the sparsity and error analysis"]]
[6, 2, []]
[8, 2, []]

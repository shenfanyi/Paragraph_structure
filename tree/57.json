In this paper we introduce the idea of improving the performance
of parametric temporal-difference (TD) learning algorithms by
selectively emphasizing or de-emphasizing their updates on
different time steps. In particular, we show that varying the
emphasis of linear TD($\lambda$)'s updates in a particular way
causes its expected update to become stable under off-policy
training. The only prior model-free TD methods to achieve this
with per- step computation linear in the number of function
approximation parameters are the gradient-TD family of methods
including TDC, GTD($\lambda$), and GQ$\lambda$). Compared to
these methods, our 

[9, 2, []]
[6, 2, []]
[14, 1, ["In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps", "The only prior model-free TD methods to achieve this with per- step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD($\\lambda$), and GQ$\\lambda$)", "Compared to these methods, our"]]
[1, 2, []]

We consider the recovery of a low rank real-valued matrix $M$
given a subset of noisy discrete (or quantized) measurements.
Such problems arise in several applications such as
collaborative filtering, learning and content analytics, and
sensor network localization. We consider constrained maximum
likelihood estimation of $M$, under a constraint on the entry-
wise infinity-norm of $M$ and an exact rank constraint. We
provide upper bounds on the Frobenius norm of matrix estimation
error under this model. Previous theoretical investigations have
focused on binary (1-bit) quantizers, and been based on convex
relaxation of the rank. Compared to the existing binary results,
our performance upper bound has faster convergence rate with
matrix dimensions when the fraction of revealed observations is
fixed. We also propose a globally convergent optimization
algorithm based on low rank factorization of $M$ and validate
the method on synthetic and real data, with improved performance
over previous methods.

[6, 2, []]
[9, 1, ["Such problems arise in several applications such as collaborative filtering, learning and content analytics, and sensor network localization"]]
[8, 1, []]
[6, 2, []]
[5, 2, []]
[8, 1, []]
[8, 1, ["We consider the recovery of a low rank real-valued matrix $M$ given a subset of noisy discrete (or quantized) measurements", "We consider constrained maximum likelihood estimation of $M$, under a constraint on the entry- wise infinity-norm of $M$ and an exact rank constraint", "Previous theoretical investigations have focused on binary (1-bit) quantizers, and been based on convex relaxation of the rank", "Compared to the existing binary results, our performance upper bound has faster convergence rate with matrix dimensions when the fraction of revealed observations is fixed", "We also propose a globally convergent optimization algorithm based on low rank factorization of $M$ and validate the method on synthetic and real data, with improved performance over previous methods"]]
